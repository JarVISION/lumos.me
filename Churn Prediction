Churn Prediction â€“ Temporal Structural Model embedded in a Logit Framework

PROBLEM STATEMENT :

To predict whether a customer would renew his/ her service contract with the company based on past incidents.


BACKGROUND FOR THE ANALYSIS :

The company is a leading desktop and laptop manufacturer.
Every customer can avail a service package for a year where for a particular fee, the customer can get the device serviced (both hardware and software) for the whole year for any problems or incidents faced with the device.
The service can be renewed for each subsequent year.
BUSINESS VALUE :

Predicting and analyzing the reasons for customer response would help in targeted marketing and customer retention strategies and improve efficiency.

RESPONSE VARIABLE : Contract renewed (1) or Lost (0)

FACTOR VARIABLES :
Renewal Date
Date of incident
No. of incidents
No. of response miss
No. of visit miss
No. of escalations

DATA CLEANING :

We had 2 datasets:
Conversion set:
Renewal Date
Status
Service Agreement ID
Chan Prof
Incident set:
Calendar year, year and month of incident
Service Agreement ID
No. of cases, No. of escalation, No. of parts used, No. of single visit missed, No. of response missed

MS Excel

Removed multiple renewals for a same ID from the conversion file
Why ? Because the percentage of multiple renewals was very less when compared to the whole data. So itâ€™s simpler and more efficient to remove the duplicates.
So now we have unique IDs , corresponding renewal status and renewal date
How were the date variables were handled ?
We need to find the time decay, which is the difference between renewal date, given in the conversion file and incident date, given in the incident file
Converted the dates into MS Excel date formats and found the difference between them in term of months, using DATEDIF()â€¦This gives us TIME DECAY
Added both response miss and visit miss to get total no. of misses from the sides of both the customer and the company.
PySpark

HOW DO WE MERGE THE TWO DATASETS BASED ON A COMMON A VARIABLE ?
Based on the logic of KEY â€“ VALUE pairs, and the MAP (by Key)â€“ REDUCE (by Value)
Advantage of converting the datasets into PySpark data frame or SQL data frame
Here our key is the Service Agreement ID
We have multiple incidents for same ID in the incident table
So we groupByKey () in the incidents data frame
We remove all incidents that occur after the renewal date (only historic data is necessary to build the model, incidents after the renewal date becomes noise)
And we join the two data frames with the common key â€“ Service Agreement ID
We select the necessary columns alone using .select() on the resultant data frame
Extract the resulting data frame into a csv using .repartition(1).write.csv
The PySpark code for Data Cleaning is as follows:

SparkSession available as â€˜sparkâ€™.

p1 = sc.textFile(â€œ/incident.txtâ€)

p1 = p1.map(lambda x: x.split(â€œ\tâ€))

p1.take(2)

[[uâ€™calendar_yearmonthâ€™, uâ€™Year_Nameâ€™, uâ€™Month_Nameâ€™, uâ€™SAIDâ€™, uâ€™number_of_Casesâ€™, uâ€™number_of_escalationâ€™, uâ€™number_of_parts_usedâ€™, uâ€™number_of_single_visit_missedâ€™, uâ€™number_of_response_missedâ€™], [uâ€™201111â€², uâ€™2011â€², uâ€™11â€™, uâ€™101261727583â€², uâ€™3â€², uâ€™0â€², uâ€™3â€², uâ€™0â€², uâ€™0â€²]]

p2 = sc.textFile(â€œ/Users/harinikannansenthil/Documents/MSBA/R/Data/INSY5392/book1.txtâ€)

p2 = p2.map(lambda x: x.split(â€œ\tâ€))

p2.take(2)

[[uâ€™renewal_dateâ€™, uâ€™statusâ€™, uâ€™Service.Agreement.Idâ€™, uâ€™Chan.Profâ€™, uâ€™year_monthâ€™, uâ€™dateâ€™], [uâ€™20130421â€², uâ€™Lossâ€™, uâ€™101261727583â€², uâ€™Directâ€™, uâ€™201304â€², uâ€™21â€™]]

header1 = p1.first()

header2 = p2.first()

p1 = p1.filter(lambda x: x != header1)

p2 = p2.filter(lambda x: x != header2)

p1.take(2)

[[uâ€™201111â€², uâ€™2011â€², uâ€™11â€™, uâ€™101261727583â€², uâ€™3â€², uâ€™0â€², uâ€™3â€², uâ€™0â€², uâ€™0â€²], [uâ€™201112â€², uâ€™2011â€², uâ€™12â€™, uâ€™101261727583â€², uâ€™4â€², uâ€™0â€², uâ€™1â€², uâ€™0â€², uâ€™0â€²]]

p2.take(2)

[[uâ€™20130421â€², uâ€™Lossâ€™, uâ€™101261727583â€², uâ€™Directâ€™, uâ€™201304â€², uâ€™21â€™], [uâ€™20130501â€², uâ€™Lossâ€™, uâ€™101318541038â€², uâ€™Directâ€™, uâ€™201305â€², uâ€™1â€²]]

from pyspark.sql import Row

p1 = p1.map(lambda x: Row(yr_mon1 = x[0], yr = x[1], mon = x[2], id1 = x[3], num_cases = x[4], num_esc = x[5], num_parts = x[6], num_visitmiss = x[7], num_respmiss = x[8]))

p2 = p2.map(lambda x: Row(ren_dt = x[0], status = x[1], id2 = x[2], chan = x[3], yr_mon2 = x[4], day = x[5]))

p1.take(2)

[Row(id1=uâ€™101261727583â€², mon=uâ€™11â€™, num_cases=uâ€™3â€², num_esc=uâ€™0â€², num_parts=uâ€™3â€², num_respmiss=uâ€™0â€², num_visitmiss=uâ€™0â€², yr=uâ€™2011â€², yr_mon1=uâ€™201111â€²), Row(id1=uâ€™101261727583â€², mon=uâ€™12â€™, num_cases=uâ€™4â€², num_esc=uâ€™0â€², num_parts=uâ€™1â€², num_respmiss=uâ€™0â€², num_visitmiss=uâ€™0â€², yr=uâ€™2011â€², yr_mon1=uâ€™201112â€²)]

p2.take(2)

[Row(chan=uâ€™Directâ€™, day=uâ€™21â€™, id2=uâ€™101261727583â€², ren_dt=uâ€™20130421â€², status=uâ€™Lossâ€™, yr_mon2=uâ€™201304â€²), Row(chan=uâ€™Directâ€™, day=uâ€™1â€², id2=uâ€™101318541038â€², ren_dt=uâ€™20130501â€², status=uâ€™Lossâ€™, yr_mon2=uâ€™201305â€²)]

df1 = p1.toDF()

df1.show(5)

df2 = p2.toDF()

df2.show(5)

sqlContext = SQLContext(sc)

df1.registerTempTable(â€œdf1tempâ€)

df2.registerTempTable(â€œdf2tempâ€)

df3 = df1.join(df2, (df1.id1 == df2.id2)).select(â€˜id1â€™, â€˜yr_mon1â€™, â€˜yr_mon2â€™, â€˜num_casesâ€™, â€˜num_escâ€™, â€˜num_partsâ€™, â€˜num_respmissâ€™, â€˜num_visitmissâ€™, â€˜statusâ€™)

df3.show(20)

df3.repartition(1).write.csv(â€˜/df5.csvâ€™)

The R code for building the model and model selection is as follows :

data <- read.csv("Proj2.csv", header = TRUE)
id <- data$Id
decay <- data$time_decay
num_cases <- data$num_cases
num_miss <- data$num_respmiss + data$num_visitmiss
num_esc <- data$num_esc
status <- data$status
data1 <- read.csv("uni_x.csv", header = TRUE)
status1 <- data1$status
x <- ifelse(status1 == "Loss", 0, 1)

df1 <- data.frame(id = id, n1 = decay, n2 = num_cases, n3 = num_miss, n4 = num_esc)
uni_id <- unique(df1$id)
length(uni_id)
length(x)



par_switch <- c(1, 1, 1, 1, 1)
par <- c(0.02, 0.02, 0.02, 0.02, 0.02)

L <- c(-Inf, -Inf, -Inf, -Inf, -Inf)
U <- c(Inf, Inf, Inf, Inf, Inf) 

for (i in 1:5){
 if(par_switch[i] == 0) L[i] = 0
 else L[i] = L[i]
}

for (i in 1:5){
 if(par_switch[i] == 0) U[i] = 0.00001
 else U[i] = U[i]
}


M <- function(par, x, df1) {
 
 y <- rep(0,length(uni_id))
 v <- rep(0, length(id))
 a <- par[1]
 b <- par[2]
 c <- par[3]
 d <- par[4]
 e <- par[5]
 p2 <- rep(-1,length(y))
 
 for( j in 1:length(uni_id)) {
 for(i in 1: length(id)) {
 if(id[i] == uni_id[j]) v[i] <- (exp(-c*df1$n1[i]))*(log(1 + df1$n2[i]))*(1 + d*(log(1 + df1$n3[i])) + e*(log(1 + df1$n4[i])))
 }
 y[j] <- sum(v)
 if(x[j] ==1) p2[j] <- (1/(1+exp(-(a+b*y[j]))))
 else p2[j] <- 1-((1/(1+exp(-(a+b*y[j])))))
 }
 return (-sum(log(p2)))
}

s <- nlminb(par, M, x = x, df1 = df1, lower = L, upper = U)
s$par
s$objective
s$convergence
s$iterations



COEFFICIENTS: (5 in total)
a, b, Î²(for time decay), Cim(for no. of misses), Cie(for no. of escalations)
Model selection was done using a switch function. So a change in switch will decide which model is being executed. (Switch 1 for keeping a term in the model and 0 for dropping it)
FULL MODEL:
Switch : (1, 1, 1, 1, 1)
Starting Values : (0, 0, 0.01, 0.01, 0.01)
Par:
a = -1.401
b = -3.13 *_ğ’†_âˆ’ğŸ•_
Î² = 0.117
Cim = 0.083
Cie = -0.0018
Objective:
7605.426

NULL MODEL:
Switch : (1, 0, 0, 0, 0)
Starting Values : (0, 0, 0.01, 0.01, 0.01)
Par:
a = -0.98
b = 0
Î² = 0
Cim = 0
Cie = 0
Objective:
7842.733

COMPARISON BETWEEN FULL MODEL AND NULL MODEL:
Null $ objective > Full $ objective
We can reject the Null and say that Full model fits significantly better than the Null model

INTERPRETATION OF THE COEFFICIENTS IN FULL MODEL:
b = -3.13 *(ğ’†)âˆ’ğŸ•
â€“ Negative
â€“ Increase in no. of incidents increases the probability of the customerâ€™s contract getting        lost in the order of b.
Î² = 0.117
â€“ Positive
â€“ Increase in time decay (more the time in between incident and renewal date) increases        the probability of the customerâ€™s contract getting converted in the order of Î².
Cim = 0.083
â€“ Positive (??!)
â€“ Increase in no. of misses increases the probability of the customerâ€™s contract getting            converted in the order of Cim
Cie = -0.0018
â€“ Negative
â€“ Increase in no. of escalations increases the probability of the customerâ€™s contract getting    lost in the order of Cie

BUSINESS IMPLICATIONS :

There are two ways to improving the process of service contract retention:
Put in money to market the product in all segments equally, no matter what the trend has been OR
Study the model and predict to a certain probability, whether a customer would renew the contract or not .
That way, we can recognized which segment of customers need rigorous marketing strategies and which segment needs less
For example, if a customer has no recent incidents in the year the contract is up for renewal, then according to our model, the customer would most probably renew their contract
This allows for informed and intelligent business decision making
